{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "O3MbrgwrAc5s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6ffUNvpx_0AX"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_path = '/datasets/mnist'\n",
        "cifar_path = '/datasets/cifar'\n",
        "flower_path = '/datasets/flowers102'"
      ],
      "metadata": {
        "id": "2TO5sPMGAheI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Bk_egDEOAmlG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "cJmuULt3AguZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cifar-100"
      ],
      "metadata": {
        "id": "CvDi10uDDrY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cifar100(batch_size = 64):\n",
        "\n",
        "  # Define the transform to apply to the data\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  # Load the CIFAR-100 dataset\n",
        "  train_dataset = torchvision.datasets.CIFAR100(cifar_path, train=True, download=True, transform=train_transform)\n",
        "  test_dataset = torchvision.datasets.CIFAR100(cifar_path, train=False, download=True, transform=test_transform)\n",
        "\n",
        "  \n",
        "  # Define the number of classes per task\n",
        "  num_classes_per_task = 20\n",
        "  num_tasks = 5\n",
        "  \n",
        "  train_task_loaders = []\n",
        "  test_task_loaders = []\n",
        "\n",
        "  for i in range(num_tasks):\n",
        "    classes = list(range(i*num_classes_per_task, (i+1)*num_classes_per_task))\n",
        "    train_task_dataset = torch.utils.data.Subset(train_dataset, [j for j in range(len(train_dataset)) if train_dataset[j][1] in classes])\n",
        "    test_task_dataset = torch.utils.data.Subset(test_dataset, [j for j in range(len(test_dataset)) if test_dataset[j][1] in classes])\n",
        "    train_loader = DataLoader(train_task_dataset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    test_loader = DataLoader(test_task_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    train_task_loaders.append(train_loader)\n",
        "    test_task_loaders.append(test_loader)\n",
        "  \n",
        "  return train_task_loaders, test_task_loaders"
      ],
      "metadata": {
        "id": "YRDsO9V0AvSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Permuted MNIST"
      ],
      "metadata": {
        "id": "4C0ZMkPFEFE4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PermuteMNISTTask:\n",
        "    def __init__(self, permutation):\n",
        "        self.permutation = permutation\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = x.view(-1, 32 * 32)\n",
        "        x = x[:, self.permutation]\n",
        "        return x.view(-1, 32, 32)\n",
        "\n",
        "def get_permuted_mnist(batch_size = 64):\n",
        "\n",
        "  # Define random permutations\n",
        "  permutations = [torch.randperm(32 * 32) for i in range(5)]\n",
        "\n",
        "  # Create transforms\n",
        "  tasks = []\n",
        "  for permutation in permutations:\n",
        "      tasks.append(transforms.Compose([\n",
        "          transforms.Grayscale(num_output_channels=3),\n",
        "          torchvision.transforms.Resize(32),\n",
        "          transforms.ToTensor(),\n",
        "          PermuteMNISTTask(permutation)\n",
        "      ]))              \n",
        "\n",
        "  train_loaders = []\n",
        "  test_loaders = []\n",
        "\n",
        "  # Create tasks    \n",
        "  for task in tasks:\n",
        "    train_dataset = torchvision.datasets.MNIST(mnist_path, train=True, download=True, transform = task)\n",
        "    test_dataset = torchvision.datasets.MNIST(mnist_path, train=False, download=True, transform = task)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    train_loaders.append(train_loader)\n",
        "    test_loaders.append(test_loader)\n",
        "\n",
        "  return train_loaders, test_loaders"
      ],
      "metadata": {
        "id": "IdKzEIt9EJT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flowers 102"
      ],
      "metadata": {
        "id": "E2krMMdpEPxQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_flowers102(batch_size = 16):\n",
        "  # Create transforms\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "  \n",
        "  train_dataset = torchvision.datasets.Flowers102(flower_path, split=\"train\", download=True, transform=train_transform)\n",
        "  test_dataset = torchvision.datasets.Flowers102(flower_path, split=\"test\", download=True, transform=test_transform)\n",
        "\n",
        "  num_classes_per_task = 17\n",
        "  num_tasks = 6\n",
        "\n",
        "  train_task_loaders = []\n",
        "  test_task_loaders = []\n",
        "  \n",
        "  # Create tasks \n",
        "  for i in range(num_tasks):\n",
        "    classes = list(range(i*num_classes_per_task, (i+1)*num_classes_per_task))\n",
        "    train_task_dataset = torch.utils.data.Subset(train_dataset, [j for j in range(len(train_dataset)) if train_dataset[j][1] in classes])\n",
        "    test_task_dataset = torch.utils.data.Subset(test_dataset, [j for j in range(len(test_dataset)) if test_dataset[j][1] in classes])\n",
        "    train_loader = DataLoader(train_task_dataset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    test_loader = DataLoader(test_task_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    train_task_loaders.append(train_loader)\n",
        "    test_task_loaders.append(test_loader)\n",
        "\n",
        "  return train_task_loaders, test_task_loaders"
      ],
      "metadata": {
        "id": "OqxxFGWgESng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diff Datasets"
      ],
      "metadata": {
        "id": "Ou2aqiFcEa9y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_diff_datasets(batch_size = 32):\n",
        "  \"\"\" Create an experiment with MNIST, CIFAR100 and Flowers102 as different tasks  \"\"\"\n",
        "\n",
        "  trainloaders = []\n",
        "  testloaders = []\n",
        "\n",
        "  #CIFAR100 and Flowers102 transforms\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "  \n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  #MNIST transform\n",
        "  mnist_train = transforms.Compose([\n",
        "      transforms.Grayscale(num_output_channels=3),\n",
        "      train_transform\n",
        "  ])\n",
        "\n",
        "  mnist_test = transforms.Compose([\n",
        "      transforms.Grayscale(num_output_channels=3),\n",
        "      test_transform\n",
        "  ])\n",
        "\n",
        "  # Load MNIST\n",
        "  mnist_train = torchvision.datasets.MNIST(mnist_path, train=True, download=True, transform = mnist_train)\n",
        "  mnist_test = torchvision.datasets.MNIST(mnist_path, train=False, download=True, transform = mnist_test)\n",
        "\n",
        "  mnist_train_loader = DataLoader(mnist_train, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  mnist_test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "  trainloaders.append(mnist_train_loader)\n",
        "  testloaders.append(mnist_test_loader)\n",
        "\n",
        "  #Load CIFAR100\n",
        "  cifar_train = torchvision.datasets.CIFAR100(cifar_path, train=True, download=True, transform=train_transform)\n",
        "  cifar_test = torchvision.datasets.CIFAR100(cifar_path, train=False, download=True, transform=test_transform)\n",
        "\n",
        "  cifar_train_loader = DataLoader(cifar_train, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  cifar_test_loader = DataLoader(cifar_test, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "  trainloaders.append(cifar_train_loader)\n",
        "  testloaders.append(cifar_test_loader)\n",
        "\n",
        "  # Load Flowers102\n",
        "  flowers_train = torchvision.datasets.Flowers102(flower_path, split=\"train\", download=True, transform=train_transform)\n",
        "  flowers_test = torchvision.datasets.Flowers102(flower_path, split=\"test\", download=True, transform=test_transform)\n",
        "\n",
        "  flowers_train_loader = DataLoader(flowers_train, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  flowers_test_loader = DataLoader(flowers_test, batch_size = batch_size, shuffle = False)\n",
        "  \n",
        "  trainloaders.append(flowers_train_loader)\n",
        "  testloaders.append(flowers_test_loader)\n",
        "\n",
        "  return trainloaders, testloaders"
      ],
      "metadata": {
        "id": "ZNwdI7fxEeBS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PNN"
      ],
      "metadata": {
        "id": "4Vb5ILkVEn9N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Blocks"
      ],
      "metadata": {
        "id": "n4No06RgErrB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgBlock(nn.Module):\n",
        "  def runBlock(self, x):\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def runLateral(self, i, x):\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def runActivation(self, x):\n",
        "    raise NotImplementedError\n",
        "  \n",
        "  def isLateralized(self):\n",
        "    return True\n",
        "\n",
        "class ProgInertBlock(ProgBlock):\n",
        "  def isLateralized(self):\n",
        "    return False"
      ],
      "metadata": {
        "id": "VDY6mtfbFUcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgDenseBlock(ProgBlock):\n",
        "  \"\"\" A ProgBlock containing a single fully connected layer(nn.Linear) \"\"\"\n",
        "\n",
        "  def __init__(self, inSize, outSize, numLaterals, activation = nn.ReLU()):\n",
        "    super().__init__()\n",
        "    self.numLaterals = numLaterals\n",
        "    self.inSize = inSize \n",
        "    self.outSize = outSize\n",
        "    self.module = nn.Linear(inSize, outSize)\n",
        "    self.laterals = nn.ModuleList([nn.Linear(inSize, outSize) for _ in range(numLaterals)])\n",
        "    if activation is None: self.activation = (lambda x : x)\n",
        "    else:                  self.activation = activation\n",
        "\n",
        "  def runBlock(self, x):\n",
        "    return self.module(x)\n",
        "\n",
        "  def runLateral(self, i, x):\n",
        "    lat = self.laterals[i]\n",
        "    return lat(x)\n",
        "  \n",
        "  def runActivation(self, x):\n",
        "    return self.activation(x)"
      ],
      "metadata": {
        "id": "F7awleqcFhQK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgDenseBNBlock(ProgBlock):\n",
        "  \"\"\" A ProgBlock containing a single fully connected layer and batch normalization \"\"\"\n",
        "  def __init__(self, inSize, outSize, numLaterals, activation = nn.ReLU(), bnArgs = dict()):\n",
        "    super().__init__()\n",
        "    self.numLaterals = numLaterals\n",
        "    self.inSize = inSize\n",
        "    self.outSize = outSize\n",
        "    self.module = nn.Linear(inSize, outSize)\n",
        "    self.moduleBN = nn.BatchNorm1d(outSize, **bnArgs)\n",
        "    self.laterals = nn.ModuleList([nn.Linear(inSize, outSize) for _ in range(numLaterals)])\n",
        "    self.lateralBNs = nn.ModuleList([nn.BatchNorm1d(outSize, **bnArgs) for _ in range(numLaterals)])\n",
        "    if activation is None: self.activation = (lambda x : x) \n",
        "    else:                  self.activation = activation\n",
        "  \n",
        "  def runBlock(self, x):\n",
        "    return self.moduleBN(self.module(x))\n",
        "  \n",
        "  def runLateral(self, i, x):\n",
        "    lat = self.laterals[i]\n",
        "    bn = self.lateralBNs[i]\n",
        "    return bn(lat(x))\n",
        "  \n",
        "  def runActivation(self, x):\n",
        "    return self.activation(x)"
      ],
      "metadata": {
        "id": "Qt3nDg3uFsOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgConv2DBlock(ProgBlock):\n",
        "  \"\"\" A ProgBlock containing a single Conv2D layer \"\"\"\n",
        "\n",
        "  def __init__(self, inSize, outSize, kernelSize, numLaterals, activation = nn.ReLU(), layerArgs = dict()):\n",
        "    super().__init__()\n",
        "    self.numLaterals = numLaterals\n",
        "    self.inSize = inSize\n",
        "    self.outSize = outSize\n",
        "    self.kernSize = kernelSize\n",
        "    self.module = nn.Conv2d(inSize, outSize, kernelSize, **layerArgs)\n",
        "    self.laterals = nn.ModuleList([nn.Conv2d(inSize, outSize, kernelSize, **layerArgs) for _ in range(numLaterals)])\n",
        "  \n",
        "    if activation is None: self.activation = (lambda x : x)\n",
        "    else:                  self.activation = activation\n",
        "  \n",
        "  def runBlock(self, x):\n",
        "    return self.module(x)\n",
        "\n",
        "  def runLateral(self, i, x):\n",
        "    lat = self.laterals[i]\n",
        "    return lat(x)\n",
        "  \n",
        "  def runActivation(self, x):\n",
        "    return self.activation(x)"
      ],
      "metadata": {
        "id": "JJwoUNo0FtNC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgConv2DBNBlock(ProgBlock):\n",
        "  \"\"\" A ProgBlock contaiing a single Conv2d and Batch Normalization \"\"\"\n",
        "\n",
        "  def __init__(self, inSize, outSize, kernelSize, numLaterals, activation = nn.ReLU(), layerArgs = dict(), bnArgs = dict()):\n",
        "        super().__init__()\n",
        "\n",
        "        self.numLaterals = numLaterals\n",
        "\n",
        "        self.inSize = inSize\n",
        "        self.outSize = outSize\n",
        "        self.kernSize = kernelSize\n",
        "        self.module = nn.Conv2d(inSize, outSize, kernelSize, **layerArgs)\n",
        "\n",
        "        self.moduleBN = nn.BatchNorm2d(outSize, **bnArgs)\n",
        "        self.laterals = nn.ModuleList([nn.Conv2d(inSize, outSize, kernelSize, **layerArgs) for _ in range(numLaterals)])\n",
        "        self.lateralBNs = nn.ModuleList([nn.BatchNorm2d(outSize, **bnArgs) for _ in range(numLaterals)])\n",
        "\n",
        "        if activation is None:   self.activation = (lambda x: x)\n",
        "        else:                    self.activation = activation\n",
        "\n",
        "\n",
        "  def runBlock(self, x):\n",
        "        return self.moduleBN(self.module(x))\n",
        "\n",
        "  def runLateral(self, i, x):\n",
        "        lat = self.laterals[i]\n",
        "        bn = self.lateralBNs[i]\n",
        "        return bn(lat(x))\n",
        "\n",
        "  def runActivation(self, x):\n",
        "        return self.activation(x)"
      ],
      "metadata": {
        "id": "MPwaywZpFyN1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgResnetBlock(ProgBlock):\n",
        "  \"\"\" A ProgBlock with two Conv2d layers with batch normalization and a skip connection \"\"\"\n",
        "\n",
        "  def __init__(self, inSize, outSize, kernelSize, numLaterals, activation = nn.ReLU(), bnArgs = dict(), stride = 1, expansion = 1,  downsample = None):\n",
        "\n",
        "      super().__init__()\n",
        "      self.numLaterals = numLaterals\n",
        "        \n",
        "      self.expansion = expansion\n",
        "      self.downsample = downsample\n",
        "\n",
        "      self.inSize = inSize\n",
        "      self.outSize = outSize\n",
        "      self.kernSize = kernelSize\n",
        "\n",
        "      self.conv1 = nn.Conv2d(inSize, outSize, kernelSize, stride = stride, padding = 1, bias = False)\n",
        "      self.bn1 = nn.BatchNorm2d(outSize)\n",
        "\n",
        "      self.conv2 = nn.Conv2d(outSize, outSize * self.expansion, kernelSize, padding = 1, bias = False)\n",
        "      self.bn2 = nn.BatchNorm2d(outSize * self.expansion)\n",
        "\n",
        "      self.laterals = nn.ModuleList([nn.Conv2d(inSize, outSize, kernelSize, stride = stride, padding = 1) for _ in range(numLaterals)])\n",
        "      self.lateralBNs = nn.ModuleList([nn.BatchNorm2d(outSize) for _ in range(numLaterals)])\n",
        "\n",
        "      if activation is None:   self.activation = (lambda x: x)\n",
        "      else:                    self.activation = activation\n",
        "\n",
        "\n",
        "  def runBlock(self, x):\n",
        "      input = x\n",
        "        \n",
        "      out = self.conv1(x)\n",
        "      out = self.bn1(out)\n",
        "      out = self.activation(out)\n",
        "\n",
        "      out = self.conv2(out)\n",
        "      out = self.bn2(out)\n",
        "\n",
        "      if self.downsample is not None:\n",
        "        input = self.downsample(x)\n",
        "\n",
        "      out += input\n",
        "\n",
        "      return out\n",
        "\n",
        "  def runLateral(self, i, x):\n",
        "      lat = self.laterals[i]\n",
        "      bn = self.lateralBNs[i]\n",
        "      return bn(lat(x))\n",
        "\n",
        "  def runActivation(self, x):\n",
        "      return self.activation(x)"
      ],
      "metadata": {
        "id": "1faw_z26F-Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgLambda(ProgInertBlock):\n",
        "  \"\"\" A ProgBlock with no lateral connections that runs input through lambda functions \"\"\"\n",
        "\n",
        "  def __init__(self, module):\n",
        "    super().__init__()\n",
        "    self.module = module\n",
        "  \n",
        "  def runBlock(self, x):\n",
        "    return self.module(x)\n",
        "  \n",
        "  def runActivation(self, x):\n",
        "    return x\n",
        "\n",
        "class ProgMaxPool(ProgInertBlock):\n",
        "  \"\"\" A ProgBlock that runs input through Max Pool  \"\"\"\n",
        "  def __init__(self, kernel_size, stride = None,  padding = 0):\n",
        "    super().__init__()\n",
        "    self.module = nn.MaxPool2d(kernel_size, stride, padding)\n",
        "\n",
        "  def runBlock(self, x):\n",
        "    return self.module(x)\n",
        "  \n",
        "  def runActivation(self, x):\n",
        "    return x\n",
        "\n",
        "class ProgAdaptiveAvgPool(ProgInertBlock):\n",
        "  \"\"\" A ProgBlock that runs input through Adaptive Average Pool \"\"\"\n",
        "  def __init__(self, h, w):\n",
        "    super().__init__()\n",
        "    self.module = nn.AdaptiveAvgPool2d((h, w))\n",
        "\n",
        "  def runBlock(self, x):\n",
        "    return self.module(x)\n",
        "  \n",
        "  def runActivation(self, x):\n",
        "    return x "
      ],
      "metadata": {
        "id": "C_4kE3UkGB3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Column"
      ],
      "metadata": {
        "id": "JkNNC1R2GHYX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgColumn(nn.Module):\n",
        "  \"\"\"\n",
        "  One of the PNN's columns. Outputs are stored to be available for next columns.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, colID, blockList, parentCols = []):\n",
        "    super().__init__()\n",
        "    self.colID = colID\n",
        "    self.isFrozen = False\n",
        "    self.parentCols = parentCols\n",
        "    self.blocks = nn.ModuleList(blockList)\n",
        "    self.numRows = len(blockList)\n",
        "    self.lastOutputList = []\n",
        "  \n",
        "  def freeze(self, unfreeze = False):\n",
        "    if not unfreeze: \n",
        "      self.isFrozen = True\n",
        "      for param in self.parameters(): param.requires_grad = False\n",
        "    else:\n",
        "      self.isFrozen = False\n",
        "      for param in self.parameters: param.requires_grad = True\n",
        "  \n",
        "  def forward(self, input):\n",
        "    outputs = []\n",
        "    x = input\n",
        "    for row, block in enumerate(self.blocks):\n",
        "      currOutput = block.runBlock(x)\n",
        "      if row == 0 or len(self.parentCols) < 1 or not block.isLateralized():\n",
        "        y = block.runActivation(currOutput)\n",
        "      else: \n",
        "        for c, col in enumerate(self.parentCols):\n",
        "          currOutput += block.runLateral(c, col.lastOutputList[row - 1])\n",
        "        y = block.runActivation(currOutput)\n",
        "      outputs.append(y)\n",
        "      x = y\n",
        "    self.lastOutputList = outputs\n",
        "    return outputs[-1]"
      ],
      "metadata": {
        "id": "JJ8Kdi5UGI0u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProgColumnGenerator:\n",
        "  \"\"\" Class that generates automatically new columns with the same architecture \"\"\"\n",
        "\n",
        "  def generatorColumn(self, parentCols, msg = None):\n",
        "    raise NotImplementedError\n",
        "  \n",
        "\n",
        "class Resnet18Generator(ProgColumnGenerator):\n",
        "  \"\"\" ResNet-18 architecture columns \"\"\"\n",
        "\n",
        "  def __init__(self, num_classes):\n",
        "    self.ids = 0\n",
        "    self.num_classes = num_classes\n",
        "  \n",
        "  def generateColumn(self, parentCols, msg = None):\n",
        "    cols = []\n",
        "    cols.append(ProgConv2DBNBlock(3, 64, 7, 0, activation = nn.ReLU(), layerArgs = {\"stride\" : 2, \"padding\" : 3, \"bias\" : False}))\n",
        "    cols.append(ProgMaxPool(2, 2))\n",
        "\n",
        "    cols.append(ProgResnetBlock(64, 64, 3, len(parentCols), activation = nn.ReLU()))\n",
        "    cols.append(ProgResnetBlock(64, 64, 3, len(parentCols), activation = nn.ReLU()))\n",
        "\n",
        "    #downsample\n",
        "    ds1 = self.downsample = nn.Sequential(\n",
        "      nn.Conv2d(64, 128, kernel_size=1, stride= 2, bias=False),\n",
        "      nn.BatchNorm2d(128),\n",
        "    )\n",
        "\n",
        "    cols.append(ProgResnetBlock(64, 128, 3, len(parentCols), activation = nn.ReLU(), stride = 2, downsample = ds1))\n",
        "    cols.append(ProgResnetBlock(128, 128, 3, len(parentCols), activation = nn.ReLU()))\n",
        "\n",
        "    #downsample\n",
        "    ds2 = self.downsample = nn.Sequential(\n",
        "      nn.Conv2d(128, 256, kernel_size=1, stride= 2, bias=False),\n",
        "      nn.BatchNorm2d(256),\n",
        "    )\n",
        "\n",
        "    cols.append(ProgResnetBlock(128, 256, 3, len(parentCols), activation = nn.ReLU(), stride = 2, downsample = ds2))\n",
        "    cols.append(ProgResnetBlock(256, 256, 3, len(parentCols), activation = nn.ReLU()))\n",
        "\n",
        "    #downsample\n",
        "    ds3 = self.downsample = nn.Sequential(\n",
        "      nn.Conv2d(256, 512, kernel_size=1, stride= 2, bias=False),\n",
        "      nn.BatchNorm2d(512),\n",
        "    )\n",
        "\n",
        "    cols.append(ProgResnetBlock(256, 512, 3, len(parentCols), activation = nn.ReLU(), stride = 2, downsample = ds3))\n",
        "    cols.append(ProgResnetBlock(512, 512, 3, len(parentCols), activation = nn.ReLU()))\n",
        "   \n",
        "    cols.append(ProgAdaptiveAvgPool(1, 1))\n",
        "    cols.append(ProgLambda(lambda x : torch.flatten(x, start_dim = 1)))\n",
        "    cols.append(ProgDenseBlock(512, self.num_classes, len(parentCols), activation = None))\n",
        "\n",
        "    return ProgColumn(self.__genID(), cols, parentCols = parentCols)\n",
        "  \n",
        "  def __genID(self):\n",
        "    id = self.ids\n",
        "    self.ids += 1\n",
        "    return id "
      ],
      "metadata": {
        "id": "z0yQtA_YGMJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Net "
      ],
      "metadata": {
        "id": "lVxc7q76GZES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Pnn(nn.Module):\n",
        "  \"\"\" The main PNN class \"\"\"\n",
        "  def __init__(self, colGen = None):\n",
        "    super().__init__()\n",
        "    self.columns = nn.ModuleList()\n",
        "    self.numCols = 0\n",
        "    self.colMap = dict()\n",
        "    self.colGen = colGen\n",
        "  \n",
        "  def addColumn(self, col = None, msg = None):\n",
        "    if not col:\n",
        "      parents = [colRef for colRef in self.columns]\n",
        "      col = self.colGen.generateColumn(parents, msg)\n",
        "    col = col.to(device)\n",
        "    self.columns.append(col)\n",
        "    self.colMap[col.colID] = self.numCols\n",
        "    self.numCols += 1\n",
        "    return col.colID\n",
        "  \n",
        "  def freezeColumn(self, id):\n",
        "    col = self.columns[self.colMap[id]]\n",
        "    col.freeze()\n",
        "  \n",
        "  def freezeAllColumns(self):\n",
        "    for col in self.columns:\n",
        "      col.freeze()\n",
        "  \n",
        "  def unfreezeColumn(self, id):\n",
        "    col = self.columns[self.colMap[id]]\n",
        "    col.freeze(unfreeze = True)\n",
        "  \n",
        "  def unfreezeAllColumns(self):\n",
        "    for col in self.columns:\n",
        "      col.freeze(unfreeze = True)\n",
        "  \n",
        "  def forward(self, id, x):\n",
        "    colToOutput = self.colMap[id]\n",
        "    for i, col in enumerate(self.columns):\n",
        "      y = col(x)\n",
        "      if i == colToOutput:\n",
        "        return y \n",
        "\n",
        "  def getColumn(self, id):\n",
        "    col = self.columns[self.colMap[id]]\n",
        "    return col\n",
        "\n",
        "  def begin_task(self):\n",
        "   return self.addColumn()\n",
        "  \n",
        "  def end_task(self):\n",
        "    self.freezeAllColumns()\n",
        "  \n",
        "  def trainTask(self, lr,  epochs, trainloader, testloader, num_outputs = 10):\n",
        "\n",
        "    col = self.addColumn()\n",
        "    self.columns[-1].train() #train only the last column\n",
        "\n",
        "    optimizer = torch.optim.Adam(self.parameters(), lr = lr)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for inputs, labels in tqdm(trainloader, desc = 'Training Column {}, Epoch {}/{}'.format(col, epoch + 1, epochs)):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels % num_outputs\n",
        "       \n",
        "        output = self(col, inputs)\n",
        "        loss = F.cross_entropy(output, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    \n",
        "    self.evalTask(col, testloader, num_outputs)\n",
        "\n",
        "  def evalTask(self, task_idx, testloader, num_outputs = 10):\n",
        "    self.eval()\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, labels in tqdm(testloader, desc = 'Evaluating Task {}'.format(task_idx)):\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels % num_outputs\n",
        "        output = self(task_idx, inputs)\n",
        "\n",
        "        _, predictions = torch.max(output, dim = 1)\n",
        "        total += labels.shape[0]\n",
        "        correct += int((predictions == labels).sum()) \n",
        "\n",
        "    print(\"Col: {}, Testing Accuracy: {:.4f}\".format(task_idx, correct/total))"
      ],
      "metadata": {
        "id": "-8v168r0GdPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-100 Example"
      ],
      "metadata": {
        "id": "5GKuXsgdGvmN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_cifar100()\n",
        "num_outputs = 20\n",
        "lr = 1e-3\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "ujAz_UpVG0Mb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pnn(colGen = Resnet18Generator(num_classes = num_outputs))\n",
        "model.to(device)\n",
        "\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  model.trainTask(lr, epochs, trainloader, testloader, num_outputs = num_outputs)\n",
        "  model.freezeAllColumns()\n",
        "  "
      ],
      "metadata": {
        "id": "Z1S3BpYWKz_e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  model.evalTask(task_idx, testloader, num_outputs = num_outputs)"
      ],
      "metadata": {
        "id": "09vXneQ4K_pA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Permuted MNIST Example"
      ],
      "metadata": {
        "id": "G5u4q0bqLBGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_permuted_mnist()\n",
        "num_outputs = 10\n",
        "lr = 1e-3\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "Per6MPANLFCY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pnn(colGen =  Resnet18Generator(num_classes = num_outputs))\n",
        "model.to(device)\n",
        "\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  model.trainTask(lr, epochs, trainloader, testloader, num_outputs = num_outputs)\n",
        "  model.freezeAllColumns()"
      ],
      "metadata": {
        "id": "ZcP9I9L7LVLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  model.evalTask(task_idx, testloader, num_outputs = num_outputs)"
      ],
      "metadata": {
        "id": "upGCRVIcLnIK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flowers-102"
      ],
      "metadata": {
        "id": "43Mw2GsCLz2j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_flowers102()\n",
        "num_outputs = 17\n",
        "lr = 1e-3\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "ohrNvTPHL6-8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pnn(colGen = Resnet18Generator(num_classes = num_outputs))\n",
        "model.to(device)\n",
        "\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  model.trainTask(lr, epochs, trainloader, testloader, num_outputs = num_outputs)\n",
        "  model.freezeAllColumns()"
      ],
      "metadata": {
        "id": "5IuIJCaxMBdU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  model.evalTask(task_idx, testloader, num_outputs = num_outputs)"
      ],
      "metadata": {
        "id": "msGV4CoBMItB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diff Datasets "
      ],
      "metadata": {
        "id": "cTUEuGswMPBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_diff_datasets()\n",
        "num_outputs = [10, 100, 102]\n",
        "lr = 1e-3\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "VFYMAodZMR2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 0\n",
        "\n",
        "model = Pnn(colGen = Resnet18Generator(num_classes = num_outputs[task_idx]))\n",
        "model.to(device)\n",
        "\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  model.trainTask(lr, epochs[task_idx], trainloader, testloader, num_outputs = num_outputs[task_idx])\n",
        "  model.freezeAllColumns()\n",
        "\n",
        "  if task_idx < 2:\n",
        "    task_idx += 1\n",
        "    model.colGen.num_classes = num_outputs[task_idx]"
      ],
      "metadata": {
        "id": "po_yRdFdMWKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  model.evalTask(task_idx, testloader, num_outputs = num_outputs[task_idx])"
      ],
      "metadata": {
        "id": "y_IxaF5gMaaN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}