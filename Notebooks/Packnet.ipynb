{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports\n"
      ],
      "metadata": {
        "id": "6kbPF5rEc6hZ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODJJjePoc11f"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_path = '/datasets/mnist'\n",
        "cifar_path = '/datasets/cifar'\n",
        "flower_path = '/datasets/flowers102'\n",
        "packnet_path = 'packnet_weights.pth'"
      ],
      "metadata": {
        "id": "iA_4i6d9ec8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "PBpa076OedkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "GDFt_i0Gc_t2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-100"
      ],
      "metadata": {
        "id": "j03LdmvEeVDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cifar100(batch_size = 64):\n",
        "\n",
        "  # Define the transform to apply to the data\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  # Load the CIFAR-100 dataset\n",
        "  train_dataset = torchvision.datasets.CIFAR100(cifar_path, train=True, download=True, transform=train_transform)\n",
        "  test_dataset = torchvision.datasets.CIFAR100(cifar_path, train=False, download=True, transform=test_transform)\n",
        "\n",
        "  \n",
        "  # Define the number of classes per task\n",
        "  num_classes_per_task = 20\n",
        "  num_tasks = 5\n",
        "  \n",
        "  train_task_loaders = []\n",
        "  test_task_loaders = []\n",
        "\n",
        "  for i in range(num_tasks):\n",
        "    classes = list(range(i*num_classes_per_task, (i+1)*num_classes_per_task))\n",
        "    train_task_dataset = torch.utils.data.Subset(train_dataset, [j for j in range(len(train_dataset)) if train_dataset[j][1] in classes])\n",
        "    test_task_dataset = torch.utils.data.Subset(test_dataset, [j for j in range(len(test_dataset)) if test_dataset[j][1] in classes])\n",
        "    train_loader = DataLoader(train_task_dataset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    test_loader = DataLoader(test_task_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    train_task_loaders.append(train_loader)\n",
        "    test_task_loaders.append(test_loader)\n",
        "  \n",
        "  return train_task_loaders, test_task_loaders"
      ],
      "metadata": {
        "id": "QGF7JJoZdHsd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Permuted MNIST"
      ],
      "metadata": {
        "id": "FpNLZGHzemGo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PermuteMNISTTask:\n",
        "    def __init__(self, permutation):\n",
        "        self.permutation = permutation\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = x.view(-1, 32 * 32)\n",
        "        x = x[:, self.permutation]\n",
        "        return x.view(-1, 32, 32)\n",
        "\n",
        "def get_permuted_mnist(batch_size = 64):\n",
        "\n",
        "  # Define random permutations\n",
        "  permutations = [torch.randperm(32 * 32) for i in range(5)]\n",
        "\n",
        "  # Create transforms\n",
        "  tasks = []\n",
        "  for permutation in permutations:\n",
        "      tasks.append(transforms.Compose([\n",
        "          transforms.Grayscale(num_output_channels=3),\n",
        "          torchvision.transforms.Resize(32),\n",
        "          transforms.ToTensor(),\n",
        "          PermuteMNISTTask(permutation)\n",
        "      ]))              \n",
        "\n",
        "  train_loaders = []\n",
        "  test_loaders = []\n",
        "\n",
        "  # Create tasks    \n",
        "  for task in tasks:\n",
        "    train_dataset = torchvision.datasets.MNIST(mnist_path, train=True, download=True, transform = task)\n",
        "    test_dataset = torchvision.datasets.MNIST(mnist_path, train=False, download=True, transform = task)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    train_loaders.append(train_loader)\n",
        "    test_loaders.append(test_loader)\n",
        "\n",
        "  return train_loaders, test_loaders"
      ],
      "metadata": {
        "id": "dzt3iJUlen6h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flowers-102"
      ],
      "metadata": {
        "id": "j3yoWNdneu0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_flowers102(batch_size = 16):\n",
        "  # Create transforms\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "  \n",
        "  train_dataset = torchvision.datasets.Flowers102(flower_path, split=\"train\", download=True, transform=train_transform)\n",
        "  test_dataset = torchvision.datasets.Flowers102(flower_path, split=\"test\", download=True, transform=test_transform)\n",
        "\n",
        "  num_classes_per_task = 17\n",
        "  num_tasks = 6\n",
        "\n",
        "  train_task_loaders = []\n",
        "  test_task_loaders = []\n",
        "  \n",
        "  # Create tasks \n",
        "  for i in range(num_tasks):\n",
        "    classes = list(range(i*num_classes_per_task, (i+1)*num_classes_per_task))\n",
        "    train_task_dataset = torch.utils.data.Subset(train_dataset, [j for j in range(len(train_dataset)) if train_dataset[j][1] in classes])\n",
        "    test_task_dataset = torch.utils.data.Subset(test_dataset, [j for j in range(len(test_dataset)) if test_dataset[j][1] in classes])\n",
        "    train_loader = DataLoader(train_task_dataset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    test_loader = DataLoader(test_task_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    train_task_loaders.append(train_loader)\n",
        "    test_task_loaders.append(test_loader)\n",
        "\n",
        "  return train_task_loaders, test_task_loaders"
      ],
      "metadata": {
        "id": "PwlBj2u5ewl5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diff Datasets"
      ],
      "metadata": {
        "id": "dvBBAEAHe9h1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_diff_datasets(batch_size = 32):\n",
        "  \"\"\" Create an experiment with MNIST, CIFAR100 and Flowers102 as different tasks  \"\"\"\n",
        "\n",
        "  trainloaders = []\n",
        "  testloaders = []\n",
        "\n",
        "  #CIFAR100 and Flowers102 transforms\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "  \n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  #MNIST transform\n",
        "  mnist_train = transforms.Compose([\n",
        "      transforms.Grayscale(num_output_channels=3),\n",
        "      train_transform\n",
        "  ])\n",
        "\n",
        "  mnist_test = transforms.Compose([\n",
        "      transforms.Grayscale(num_output_channels=3),\n",
        "      test_transform\n",
        "  ])\n",
        "\n",
        "  # Load MNIST\n",
        "  mnist_train = torchvision.datasets.MNIST(mnist_path, train=True, download=True, transform = mnist_train)\n",
        "  mnist_test = torchvision.datasets.MNIST(mnist_path, train=False, download=True, transform = mnist_test)\n",
        "\n",
        "  mnist_train_loader = DataLoader(mnist_train, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  mnist_test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "  trainloaders.append(mnist_train_loader)\n",
        "  testloaders.append(mnist_test_loader)\n",
        "\n",
        "  #Load CIFAR100\n",
        "  cifar_train = torchvision.datasets.CIFAR100(cifar_path, train=True, download=True, transform=train_transform)\n",
        "  cifar_test = torchvision.datasets.CIFAR100(cifar_path, train=False, download=True, transform=test_transform)\n",
        "\n",
        "  cifar_train_loader = DataLoader(cifar_train, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  cifar_test_loader = DataLoader(cifar_test, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "  trainloaders.append(cifar_train_loader)\n",
        "  testloaders.append(cifar_test_loader)\n",
        "\n",
        "  # Load Flowers102\n",
        "  flowers_train = torchvision.datasets.Flowers102(flower_path, split=\"train\", download=True, transform=train_transform)\n",
        "  flowers_test = torchvision.datasets.Flowers102(flower_path, split=\"test\", download=True, transform=test_transform)\n",
        "\n",
        "  flowers_train_loader = DataLoader(flowers_train, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  flowers_test_loader = DataLoader(flowers_test, batch_size = batch_size, shuffle = False)\n",
        "  \n",
        "  trainloaders.append(flowers_train_loader)\n",
        "  testloaders.append(flowers_test_loader)\n",
        "\n",
        "  return trainloaders, testloaders\n"
      ],
      "metadata": {
        "id": "IQDwK_0sfBW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Packnet"
      ],
      "metadata": {
        "id": "JrN3u-eydIEn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network"
      ],
      "metadata": {
        "id": "EsRr7y_EfS3l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PacknetResNet(nn.Module):\n",
        "  \"\"\"ResNet-50\"\"\"\n",
        "\n",
        "  def __init__(self, make_model = True):\n",
        "    super(PacknetResNet, self).__init__()\n",
        "\n",
        "    resnet = models.resnet50(weights = None)\n",
        "    \n",
        "    self.datasets = [] \n",
        "    self.classifiers = nn.ModuleList()\n",
        "    self.shared = nn.Sequential()\n",
        "   \n",
        "    for name, module in resnet.named_children():\n",
        "      if name != 'fc':\n",
        "        self.shared.add_module(name, module)\n",
        "        \n",
        "    #model.set_dataset() has to be called explicity, else model won't work\n",
        "    self.classifier = None\n",
        "\n",
        "  def train_nobn(self, mode = True):\n",
        "    \"\"\"Override the default module train\"\"\"\n",
        "    super(PacknetResNet, self).train(mode)\n",
        "\n",
        "    # Set the BNs to eval mode so that the running means and averages do not update\n",
        "    for module in self.shared.modules():\n",
        "      if 'BatchNorm' in str(type(module)):\n",
        "        module.eval()\n",
        "  \n",
        "  def add_dataset(self, dataset, num_outputs):\n",
        "    \"\"\"Adds a new dataset and a new classifier to train for the new task. \"\"\"\n",
        "    if dataset not in self.datasets:\n",
        "      print(\"Adding new dataset : {} with number of outputs : {}\".format(dataset, num_outputs))\n",
        "      self.datasets.append(dataset)\n",
        "      self.classifiers.append(nn.Linear(2048, num_outputs))\n",
        "\n",
        "  def set_dataset(self, dataset):\n",
        "    \"\"\"Change the active classifier\"\"\"\n",
        "    assert dataset in self.datasets\n",
        "    self.classifier = self.classifiers[self.datasets.index(dataset)]\n",
        "    print(\"Setting dataset : {} with classifier : {}\".format(dataset, self.classifier))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.shared(x)\n",
        "    x = x.view(x.size(0), - 1)\n",
        "    x = self.classifier(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "r8T8XfyLdWpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pruner"
      ],
      "metadata": {
        "id": "sMGpRDsJfajv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SparsePruner(object):\n",
        "  \"\"\"Performs pruning on the given model\"\"\"\n",
        "\n",
        "  def __init__(self, model, prune_perc, previous_masks, train_bias = False, train_bn = False):\n",
        "    self.model = model \n",
        "    self.prune_perc = prune_perc\n",
        "    self.train_bias = train_bias\n",
        "    self.train_bn = train_bn\n",
        "\n",
        "    self.current_masks = None\n",
        "    self.previous_masks = previous_masks\n",
        "    valid_key = list(previous_masks.keys())[0]\n",
        "    self.current_dataset_idx = previous_masks[valid_key].max()\n",
        "  \n",
        "  def pruning_mask(self, weights, previous_mask, layer_idx):\n",
        "    \"\"\"Ranks weights by magnitude. Sets all below kth to 0.Returns pruned mask\"\"\"\n",
        "\n",
        "    # Select all prunable weights, i.e. belonging to the current dataset.\n",
        "    previous_mask = previous_mask.to(device)\n",
        "    prunable_weights = weights[previous_mask.eq(self.current_dataset_idx.to(device))]\n",
        "    abs_prunable = prunable_weights.abs()\n",
        "    cutoff_rank = round(self.prune_perc * prunable_weights.numel())\n",
        "    cutoff_value = abs_prunable.view(-1).cpu().kthvalue(cutoff_rank)[0].item()\n",
        "\n",
        "    #Remove those weights which are below cutoff and belong to\n",
        "    #the current dataset that we are training for.\n",
        "    remove_mask = weights.abs().le(cutoff_value) * previous_mask.eq(self.current_dataset_idx.to(device))\n",
        "\n",
        "    \n",
        "    previous_mask[remove_mask.eq(1)] = 0\n",
        "    mask = previous_mask\n",
        "    print('Layer #%d, pruned %d/%d (%.2f%%) (Total in layer: %d)' %\n",
        "              (layer_idx, mask.eq(0).sum(), prunable_weights.numel(),\n",
        "               100 * mask.eq(0).sum() / prunable_weights.numel(), weights.numel()))\n",
        "    return mask\n",
        "  \n",
        "  def prune(self):\n",
        "    \"\"\"Gets prunning mask for each layer, based on previous masks.\n",
        "       Sets the self.current_masks to the computed pruning masks\n",
        "    \"\"\"\n",
        "    print('Pruning fo dataset idx: %d' %(self.current_dataset_idx))\n",
        "    assert not self.current_masks, 'Current mask is not empty which means that pruning is already done for this task '\n",
        "    self.current_masks = {}\n",
        "\n",
        "    print('Pruning each layer by removing %.2f%% of values' % (100 * self.prune_perc))\n",
        "\n",
        "    for module_idx, module in enumerate(self.model.shared.modules()):\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        mask = self.pruning_mask(module.weight.data, self.previous_masks[module_idx], module_idx)\n",
        "        self.current_masks[module_idx] = mask.to(device)\n",
        "        #Set pruned weights to 0\n",
        "        weight = module.weight.data\n",
        "        weight[self.current_masks[module_idx].eq(0)] = 0.0\n",
        "\n",
        "  def make_grads_zero(self):\n",
        "    \"\"\"Sets grads of fixed weights to 0.\"\"\"\n",
        "    assert self.current_masks\n",
        "    for module_idx, module in enumerate(self.model.shared.modules()):\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        layer_mask = self.current_masks[module_idx]\n",
        "\n",
        "        # Set grads of all weights not belonging to current dataset to 0.\n",
        "        if module.weight.grad is not None:\n",
        "          module.weight.grad.data[layer_mask.ne(self.current_dataset_idx.to(device))] = 0\n",
        "          if not self.train_bias:\n",
        "            #Biases are fixed\n",
        "            if module.bias is not None:\n",
        "              module.bias.grad.fill_(0)\n",
        "      \n",
        "      elif 'BatchNorm' in str(type(module)):\n",
        "        # Set grads of batchnorm to 0\n",
        "        if not self.train_bn:\n",
        "          module.weight.grad.data.fill_(0)\n",
        "          module.bias.grad.fill_(0)\n",
        "  \n",
        "  def make_pruned_zero(self):\n",
        "    \"\"\"Makes pruned weight 0\"\"\"\n",
        "    assert self.current_masks\n",
        "\n",
        "    for module_idx, module in enumerate(self.model.shared.modules()):\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        layer_mask = self.current_masks[module_idx]\n",
        "        module.weight.data[layer_mask.eq(0)] = 0.0\n",
        "\n",
        "  def apply_mask(self, dataset_idx):\n",
        "    \"\"\"Applies mask for a specific task for evaluation\"\"\"\n",
        "    for module_idx, module in enumerate(self.model.shared.modules()):\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        weight = module.weight.data\n",
        "        mask = self.previous_masks[module_idx].to(device)\n",
        "        weight[mask.eq(0)] = 0.0\n",
        "        weight[mask.gt(dataset_idx)] = 0.0\n",
        "  \n",
        "  def restore_biases(self, biases):\n",
        "    \"\"\"Use the given biases to replace existing biases\"\"\"\n",
        "    for module_idx, module in enumerate(self.model.shared.modules()):\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        if module.bias is not None:\n",
        "          module.bias.data.copy_(biases[module_idx])\n",
        "\n",
        "  def get_biases(self):\n",
        "    \"\"\"Gets a copy of the current biases\"\"\"\n",
        "    biases = {}\n",
        "    for module_idx, module in enumerate(self.model.shared.modules()):\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        if module.bias is not None:\n",
        "          biases[module_idx] = module.bias.data.clone()\n",
        "    return biases\n",
        "  \n",
        "  def make_finetuning_mask(self):\n",
        "    \"\"\"Turns previously pruned weights (id = 0) into trainable weights for the current dataset (id = current_dataset_id)\"\"\"\n",
        "    assert self.previous_masks\n",
        "    self.current_dataset_idx += 1\n",
        "    print(\"Current Dataset Idx:\", self.current_dataset_idx)\n",
        "\n",
        "    for module_idx, module in enumerate(self.model.shared.modules()):\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        mask = self.previous_masks[module_idx]\n",
        "        mask[mask.eq(0)] = self.current_dataset_idx\n",
        "    \n",
        "    self.current_masks = self.previous_masks"
      ],
      "metadata": {
        "id": "6bYb1fhafff6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main Class"
      ],
      "metadata": {
        "id": "rp9L3OS6foas"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Packnet(object):\n",
        "\n",
        "  def __init__(self, args, model, previous_masks, trainloader, testloader):\n",
        "    self.args = args \n",
        "    self.model = model\n",
        "\n",
        "    self.train_data_loader = trainloader\n",
        "    self.test_data_loader = testloader\n",
        "\n",
        "    self.criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    self.pruner = SparsePruner(\n",
        "        self.model, self.args['prune_perc_per_layer'], previous_masks, \n",
        "        self.args['train_biases'], self.args['train_bn'])\n",
        "\n",
        "  def train(self, epochs, optimizer):\n",
        "\n",
        "    self.model.to(device)\n",
        "\n",
        "    # Train batch normalization from the first task. Next tasks use the same stats. !!!!!!\n",
        "    if self.pruner.current_dataset_idx == 1:\n",
        "      self.model.train()\n",
        "    else:\n",
        "      self.model.train_nobn()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "      for inputs, label in tqdm(self.train_data_loader, desc = 'Epoch {}/{}'.format(epoch + 1, epochs)):\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "        label = label.to(device)\n",
        "        label = label % self.args['num_outputs']\n",
        "\n",
        "        self.model.zero_grad()\n",
        "\n",
        "        #Do forward-backward\n",
        "        output = self.model(inputs)\n",
        "        self.criterion(output, label).backward()\n",
        "\n",
        "        #Set fixed param grads to 0.\n",
        "        self.pruner.make_grads_zero()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        #Set pruned weights to 0.\n",
        "        self.pruner.make_pruned_zero()\n",
        "        \n",
        "\n",
        "    self.eval(self.pruner.current_dataset_idx)\n",
        "    print('Finished Training...')\n",
        "    print('-' * 16)\n",
        "\n",
        "\n",
        "  def eval(self, dataset_idx, biases = None):\n",
        "\n",
        "    self.pruner.apply_mask(dataset_idx)\n",
        "    if biases is not None:\n",
        "      self.pruner.restore_biases(biases)\n",
        "    \n",
        "    self.model.eval()\n",
        "    eval_total = 0\n",
        "    eval_correct = 0\n",
        "    with torch.no_grad():\n",
        "      for inputs, label in tqdm(self.test_data_loader, desc = 'Evaluation on task {}'.format(dataset_idx)):\n",
        "        inputs = inputs.to(device)\n",
        "        label = label.to(device)\n",
        "        label = label % self.args['num_outputs']\n",
        "\n",
        "        output = self.model(inputs)\n",
        "\n",
        "        _, predictions = torch.max(output, dim = 1)\n",
        "        eval_total += label.shape[0]\n",
        "        eval_correct += int((predictions == label).sum())\n",
        "      print(\"Valdation Accuracy: {:.4f}\"\n",
        "       .format(eval_correct / eval_total))\n",
        "\n",
        "  def prune(self):\n",
        "    print('Pre-prune evaluation:')\n",
        "    self.eval(self.pruner.current_dataset_idx)\n",
        "\n",
        "    self.pruner.prune()\n",
        "    self.check(True)\n",
        "\n",
        "    print(\"\\nPost-prune evaluation:\")\n",
        "    self.eval(self.pruner.current_dataset_idx)\n",
        "    \n",
        "    if self.args['post_prune_epochs']:\n",
        "      print('Doing post prune training...')\n",
        "  \n",
        "      optimizer = optim.Adam(self.model.parameters(), lr = self.args['lr'])\n",
        "      self.train(self.args['post_prune_epochs'], optimizer)\n",
        "    \n",
        "    print('-' * 16)\n",
        "    print('Pruning summary:')\n",
        "    self.check(True)\n",
        "    \n",
        "\n",
        "  def check(self, verbose = True):\n",
        "    \"\"\"Checks what percent of each layer is pruned\"\"\"\n",
        "    print('Checking Network...')\n",
        "    for layer_idx, module in enumerate(self.model.shared.modules()):\n",
        "      if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "        weight = module.weight.data\n",
        "        num_params = weight.numel()\n",
        "        num_zero = weight.view(-1).eq(0).sum()\n",
        "        if verbose:\n",
        "           print('Layer #%d: Pruned %d/%d (%.2f%%)' %\n",
        "                 (layer_idx, num_zero, num_params, 100 * num_zero / num_params))\n",
        "  \n",
        "  def save_model(self, savename):\n",
        "        \"\"\"Saves model to file.\"\"\"\n",
        "        base_model = self.model\n",
        "\n",
        "        # Prepare the ckpt.\n",
        "        ckpt = {\n",
        "            'args': self.args,\n",
        "            'previous_masks': self.pruner.current_masks,\n",
        "            'model': base_model,\n",
        "        }\n",
        "\n",
        "        # Save to file.\n",
        "        torch.save(ckpt, savename)\n",
        "  \n",
        "  def train_task(self):\n",
        "    self.pruner.make_finetuning_mask()\n",
        "    optimizer = optim.Adam(self.model.parameters(), lr = self.args['lr'])\n",
        "\n",
        "    #Perform finetunning\n",
        "    self.train(self.args['finetune_epochs'], optimizer)"
      ],
      "metadata": {
        "id": "thNJl78_ftWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-100 Example"
      ],
      "metadata": {
        "id": "4lN20pYUdXIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_cifar100()\n",
        "num_outputs = 20\n",
        "lr = 1e-3\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "eD7g5u9Idbfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PacknetResNet()\n",
        "previous_masks = {}\n",
        "\n",
        "# Filling masks with 0s, so every parameter is available for the first task\n",
        "for module_idx, module in enumerate(model.shared.modules()):\n",
        "  if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "    mask = torch.ByteTensor(module.weight.data.size()).fill_(0)\n",
        "    mask = mask.to(device)\n",
        "    previous_masks[module_idx] = mask"
      ],
      "metadata": {
        "id": "vk3iXk3ihcAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = {\"num_outputs\" : num_outputs, \"lr\" : lr,  \"finetune_epochs\" : 7, \"dataset\" : \"CIFAR100_\", \n",
        "              \"prune_perc_per_layer\" : 0.7, \"train_biases\" : False, \"train_bn\" : False}\n",
        "\n",
        "prune_args = {\"num_outputs\" : num_outputs, \"lr\" : lr * 0.1, \"dataset\" : \"CIFAR100_\", \n",
        "              \"prune_perc_per_layer\" : 0.7, \"post_prune_epochs\" : 3, \"train_biases\" : False, \"train_bn\" : False}"
      ],
      "metadata": {
        "id": "gdW5QEOQhf8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 1\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  \n",
        "  #train\n",
        "  packnet = Packnet(train_args, model, previous_masks, trainloader, testloader)\n",
        "  model.add_dataset(train_args['dataset'] + str(task_idx), train_args['num_outputs'])\n",
        "  model.set_dataset(train_args['dataset'] + str(task_idx))\n",
        "  model.to(device)\n",
        "  packnet.train_task()\n",
        "\n",
        "  #prune\n",
        "  previous_masks = packnet.pruner.current_masks\n",
        "  packnet = Packnet(prune_args, model, previous_masks, trainloader, testloader)\n",
        "  packnet.prune()\n",
        "\n",
        "  task_idx += 1\n",
        "\n",
        "  previous_masks = packnet.pruner.current_masks\n",
        "\n",
        "packnet.save_model(packnet_path)"
      ],
      "metadata": {
        "id": "wiPbLM_Rhkmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 1\n",
        "for trainloader , testloader in zip(trainloaders, testloaders):\n",
        "  ckpt = torch.load(packnet_path)\n",
        "  model = ckpt['model']\n",
        "  previous_masks = ckpt['previous_masks']\n",
        "\n",
        "  model.set_dataset(train_args['dataset'] + str(task_idx))\n",
        "  model.to(device)\n",
        "\n",
        "  packnet = Packnet(train_args, model, previous_masks, trainloader, testloader)\n",
        "\n",
        "  packnet.eval(task_idx)\n",
        "\n",
        "  task_idx += 1"
      ],
      "metadata": {
        "id": "Q_q2Ad2thxEw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Permuted MNIST Example"
      ],
      "metadata": {
        "id": "lMXg743Hdb--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_permuted_mnist()\n",
        "num_outputs = 10\n",
        "lr = 1e-3\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "spL-OAGKdfhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PacknetResNet()\n",
        "previous_masks = {}\n",
        "\n",
        "# Filling masks with 0s, so every parameter is available for the first task\n",
        "for module_idx, module in enumerate(model.shared.modules()):\n",
        "  if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "    mask = torch.ByteTensor(module.weight.data.size()).fill_(0)\n",
        "    mask = mask.to(device)\n",
        "    previous_masks[module_idx] = mask"
      ],
      "metadata": {
        "id": "2T646ERAh5CA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = {\"num_outputs\" : num_ouputs, \"lr\" : lr,  \"finetune_epochs\" : 3, \"dataset\" : \"PMNIST_\", \n",
        "              \"prune_perc_per_layer\" : 0.7, \"train_biases\" : False, \"train_bn\" : False}\n",
        "\n",
        "prune_args = {\"num_outputs\" : num_outputs, \"lr\" : lr * 0.1, \"dataset\" : \"PMNIST_\", \n",
        "              \"prune_perc_per_layer\" : 0.7, \"post_prune_epochs\" : 2,  \"train_biases\" : False, \"train_bn\" : False}"
      ],
      "metadata": {
        "id": "IrDpAw8Rh9zT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 1\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  #train\n",
        "  packnet = Packnet(train_args, model, previous_masks, trainloader, testloader)\n",
        "  model.add_dataset(train_args['dataset'] + str(task_idx), train_args['num_outputs'])\n",
        "  model.set_dataset(train_args['dataset'] + str(task_idx))\n",
        "  model.to(device)\n",
        "  packnet.train_task()\n",
        "\n",
        "  #prune\n",
        "  previous_masks = packnet.pruner.current_masks\n",
        "  packnet = Packnet(prune_args, model, previous_masks, trainloader, testloader)\n",
        "  packnet.prune()\n",
        "\n",
        "  task_idx += 1\n",
        "\n",
        "  previous_masks = packnet.pruner.current_masks\n",
        "\n",
        "packnet.save_model(packnet_path)"
      ],
      "metadata": {
        "id": "bf3RcfReiRHU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 1\n",
        "for trainloader , testloader in zip(trainloaders, testloaders):\n",
        "  ckpt = torch.load(packnet_path)\n",
        "  model = ckpt['model']\n",
        "  previous_masks = ckpt['previous_masks']\n",
        "\n",
        "  model.set_dataset(train_args['dataset'] + str(task_idx))\n",
        "  model.to(device)\n",
        "\n",
        "  packnet = Packnet(train_args, model, previous_masks, trainloader, testloader)\n",
        "\n",
        "  \n",
        "  packnet.eval(task_idx)\n",
        "\n",
        "  task_idx += 1"
      ],
      "metadata": {
        "id": "WQUDEYreiXHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flowers-102 Example"
      ],
      "metadata": {
        "id": "axWOy9EEdf_H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_flowers102()\n",
        "num_outputs = 17\n",
        "lr = 1e-3\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "dlZ6tUuQdjOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PacknetResNet()\n",
        "previous_masks = {}\n",
        "\n",
        "# Filling masks with 0s, so every parameter is available for the first task\n",
        "for module_idx, module in enumerate(model.shared.modules()):\n",
        "  if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "    mask = torch.ByteTensor(module.weight.data.size()).fill_(0)\n",
        "    mask = mask.to(device)\n",
        "    previous_masks[module_idx] = mask"
      ],
      "metadata": {
        "id": "0l5UO0N4ilhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_args = {\"num_outputs\" : num_outputs, \"lr\" : lr,  \"finetune_epochs\" : 15, \"dataset\" : \"Flowers_\", \n",
        "              \"prune_perc_per_layer\" : 0.7, \"train_biases\" : False, \"train_bn\" : False}\n",
        "\n",
        "prune_args = {\"num_outputs\" : num_outputs, \"lr\" : lr * 0.1, \"dataset\" : \"Flowers_\", \n",
        "              \"prune_perc_per_layer\" : 0.7, \"post_prune_epochs\" : 5,  \"train_biases\" : False, \"train_bn\" : False}"
      ],
      "metadata": {
        "id": "uYYEDYwviq3q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 1\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  #train\n",
        "  packnet = Packnet(train_args, model, previous_masks, trainloader, testloader)\n",
        "  model.add_dataset(train_args['dataset'] + str(task_idx), train_args['num_outputs'])\n",
        "  model.set_dataset(train_args['dataset'] + str(task_idx))\n",
        "  model.to(device)\n",
        "  packnet.train_task()\n",
        "\n",
        "  #prune\n",
        "  previous_masks = packnet.pruner.current_masks\n",
        "  packnet = Packnet(prune_args, model, previous_masks, trainloader, testloader)\n",
        "  packnet.prune()\n",
        "\n",
        "  task_idx += 1\n",
        "\n",
        "  previous_masks = packnet.pruner.current_masks\n",
        "\n",
        "packnet.save_model(packnet_path)"
      ],
      "metadata": {
        "id": "mHWt_XE7ir0q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 1\n",
        "for trainloader , testloader in zip(trainloaders, testloaders):\n",
        "  ckpt = torch.load(packnet_path)\n",
        "  model = ckpt['model']\n",
        "  previous_masks = ckpt['previous_masks']\n",
        "\n",
        "  model.set_dataset(train_args['dataset'] + str(task_idx))\n",
        "  model.to(device)\n",
        "\n",
        "  packnet = Packnet(train_args, model, previous_masks, trainloader, testloader)\n",
        "  packnet.eval(task_idx)\n",
        "\n",
        "  task_idx += 1"
      ],
      "metadata": {
        "id": "ABu0jGaliwB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diff Datasets Example"
      ],
      "metadata": {
        "id": "blxdz59udjvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_diff_datasets()\n",
        "num_outputs = [10, 100, 102]\n",
        "lr = 1e-3\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "wVKM171qdpqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = PacknetResNet()\n",
        "previous_masks = {}\n",
        "\n",
        "# Filling masks with 0s, so every parameter is available for the first task\n",
        "for module_idx, module in enumerate(model.shared.modules()):\n",
        "  if isinstance(module, nn.Conv2d) or isinstance(module, nn.Linear):\n",
        "    mask = torch.ByteTensor(module.weight.data.size()).fill_(0)\n",
        "    mask = mask.to(device)\n",
        "    previous_masks[module_idx] = mask"
      ],
      "metadata": {
        "id": "cpsS686hjEBi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datasets = ['MNIST', 'CIFAR100', 'FLOWERS102']\n",
        "\n",
        "train_args = {\"lr\" : lr, \"finetune_epochs\" : 7,  \"prune_perc_per_layer\" : 0.7, \"train_biases\" : False, \"train_bn\" : False}\n",
        "prune_args = {\"lr\" : lr * 0.1, \"post_prune_epochs\": 3,  \"prune_perc_per_layer\" : 0.7, \"train_biases\" : False, \"train_bn\" : False}\n"
      ],
      "metadata": {
        "id": "mLDLD5ZUjSRF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 1\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "   \n",
        "  train_args['num_outputs'] = num_outputs[task_idx - 1]\n",
        "  train_args['dataset'] = datasets[task_idx - 1]\n",
        "  \n",
        "  #train\n",
        "  packnet = Packnet(train_args, model, previous_masks, trainloader, testloader)\n",
        "  model.add_dataset(train_args['dataset'], train_args['num_outputs'])\n",
        "  model.set_dataset(train_args['dataset'])\n",
        "  model.to(device)\n",
        "  packnet.train_task()\n",
        "\n",
        "  #prune\n",
        "  prune_args['num_outputs'] = num_outputs[task_idx - 1]\n",
        "  prune_args['dataset'] = datasets[task_idx - 1]\n",
        "\n",
        "  previous_masks = packnet.pruner.current_masks\n",
        "  packnet = Packnet(prune_args, model, previous_masks, trainloader, testloader)\n",
        "  packnet.prune()\n",
        "\n",
        "  task_idx += 1\n",
        "\n",
        "  previous_masks = packnet.pruner.current_masks\n",
        "\n",
        "packnet.save_model(packnet_path)"
      ],
      "metadata": {
        "id": "JKI34bAJjYss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "task_idx = 1\n",
        "for trainloader , testloader in zip(trainloaders, testloaders):\n",
        "  ckpt = torch.load(packnet_path)\n",
        "  model = ckpt['model']\n",
        "  previous_masks = ckpt['previous_masks']\n",
        "\n",
        "\n",
        "  model.set_dataset(datasets[task_idx - 1])\n",
        "  model.to(device)\n",
        "\n",
        "  train_args['num_outputs'] = num_outputs[task_idx - 1]\n",
        "  train_args['dataset'] = datasets[task_idx - 1]\n",
        "  packnet = Packnet(train_args, model, previous_masks, trainloader, testloader)\n",
        "\n",
        "  packnet.eval(task_idx)\n",
        "\n",
        "  task_idx += 1"
      ],
      "metadata": {
        "id": "WoS23dXRjg2N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}