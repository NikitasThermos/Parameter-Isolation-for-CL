{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "mgvMXmfNW9oh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_oJJpEZmW6ee"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mnist_path = '/datasets/mnist'\n",
        "cifar_path = '/datasets/cifar'\n",
        "flower_path = '/datasets/flowers102'"
      ],
      "metadata": {
        "id": "xoTIwohhXC_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "Zl3BGAI6XGGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Datasets"
      ],
      "metadata": {
        "id": "1AEzBXuIXJLr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR-100"
      ],
      "metadata": {
        "id": "pHvR9CmgXMe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cifar100(batch_size = 64):\n",
        "\n",
        "  # Define the transform to apply to the data\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  # Load the CIFAR-100 dataset\n",
        "  train_dataset = torchvision.datasets.CIFAR100(cifar_path, train=True, download=True, transform=train_transform)\n",
        "  test_dataset = torchvision.datasets.CIFAR100(cifar_path, train=False, download=True, transform=test_transform)\n",
        "\n",
        "  \n",
        "  # Define the number of classes per task\n",
        "  num_classes_per_task = 20\n",
        "  num_tasks = 5\n",
        "  \n",
        "  train_task_loaders = []\n",
        "  test_task_loaders = []\n",
        "\n",
        "  for i in range(num_tasks):\n",
        "    classes = list(range(i*num_classes_per_task, (i+1)*num_classes_per_task))\n",
        "    train_task_dataset = torch.utils.data.Subset(train_dataset, [j for j in range(len(train_dataset)) if train_dataset[j][1] in classes])\n",
        "    test_task_dataset = torch.utils.data.Subset(test_dataset, [j for j in range(len(test_dataset)) if test_dataset[j][1] in classes])\n",
        "    train_loader = DataLoader(train_task_dataset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    test_loader = DataLoader(test_task_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    train_task_loaders.append(train_loader)\n",
        "    test_task_loaders.append(test_loader)\n",
        "  \n",
        "  return train_task_loaders, test_task_loaders"
      ],
      "metadata": {
        "id": "pVNireYXXIzg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Permuted MNIST"
      ],
      "metadata": {
        "id": "SdQhq_8QXSkX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PermuteMNISTTask:\n",
        "    def __init__(self, permutation):\n",
        "        self.permutation = permutation\n",
        "\n",
        "    def __call__(self, x):\n",
        "        x = x.view(-1, 32 * 32)\n",
        "        x = x[:, self.permutation]\n",
        "        return x.view(-1, 32, 32)\n",
        "\n",
        "def get_permuted_mnist(batch_size = 64):\n",
        "\n",
        "  # Define random permutations\n",
        "  permutations = [torch.randperm(32 * 32) for i in range(5)]\n",
        "\n",
        "  # Create transforms\n",
        "  tasks = []\n",
        "  for permutation in permutations:\n",
        "      tasks.append(transforms.Compose([\n",
        "          transforms.Grayscale(num_output_channels=3),\n",
        "          torchvision.transforms.Resize(32),\n",
        "          transforms.ToTensor(),\n",
        "          PermuteMNISTTask(permutation)\n",
        "      ]))              \n",
        "\n",
        "  train_loaders = []\n",
        "  test_loaders = []\n",
        "\n",
        "  # Create tasks    \n",
        "  for task in tasks:\n",
        "    train_dataset = torchvision.datasets.MNIST(mnist_path, train=True, download=True, transform = task)\n",
        "    test_dataset = torchvision.datasets.MNIST(mnist_path, train=False, download=True, transform = task)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    train_loaders.append(train_loader)\n",
        "    test_loaders.append(test_loader)\n",
        "\n",
        "  return train_loaders, test_loaders"
      ],
      "metadata": {
        "id": "J_gJW8ujXUl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Flowers-102 "
      ],
      "metadata": {
        "id": "ay4XJ8MMXaD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_flowers102(batch_size = 16):\n",
        "  # Create transforms\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "  \n",
        "  train_dataset = torchvision.datasets.Flowers102(flower_path, split=\"train\", download=True, transform=train_transform)\n",
        "  test_dataset = torchvision.datasets.Flowers102(flower_path, split=\"test\", download=True, transform=test_transform)\n",
        "\n",
        "  num_classes_per_task = 17\n",
        "  num_tasks = 6\n",
        "\n",
        "  train_task_loaders = []\n",
        "  test_task_loaders = []\n",
        "  \n",
        "  # Create tasks \n",
        "  for i in range(num_tasks):\n",
        "    classes = list(range(i*num_classes_per_task, (i+1)*num_classes_per_task))\n",
        "    train_task_dataset = torch.utils.data.Subset(train_dataset, [j for j in range(len(train_dataset)) if train_dataset[j][1] in classes])\n",
        "    test_task_dataset = torch.utils.data.Subset(test_dataset, [j for j in range(len(test_dataset)) if test_dataset[j][1] in classes])\n",
        "    train_loader = DataLoader(train_task_dataset, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "    test_loader = DataLoader(test_task_dataset, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "    train_task_loaders.append(train_loader)\n",
        "    test_task_loaders.append(test_loader)\n",
        "\n",
        "  return train_task_loaders, test_task_loaders"
      ],
      "metadata": {
        "id": "1L9JZJ3dXZf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diff Datasets"
      ],
      "metadata": {
        "id": "vBkW1VCwXgEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_diff_datasets(batch_size = 32):\n",
        "  \"\"\" Create an experiment with MNIST, CIFAR100 and Flowers102 as different tasks  \"\"\"\n",
        "\n",
        "  trainloaders = []\n",
        "  testloaders = []\n",
        "\n",
        "  #CIFAR100 and Flowers102 transforms\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.RandomHorizontalFlip(),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "  \n",
        "  test_transform = transforms.Compose([\n",
        "      transforms.Resize([256, 256]),\n",
        "      transforms.CenterCrop(224),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761))])\n",
        "\n",
        "  #MNIST transform\n",
        "  mnist_train = transforms.Compose([\n",
        "      transforms.Grayscale(num_output_channels=3),\n",
        "      train_transform\n",
        "  ])\n",
        "\n",
        "  mnist_test = transforms.Compose([\n",
        "      transforms.Grayscale(num_output_channels=3),\n",
        "      test_transform\n",
        "  ])\n",
        "\n",
        "  # Load MNIST\n",
        "  mnist_train = torchvision.datasets.MNIST(mnist_path, train=True, download=True, transform = mnist_train)\n",
        "  mnist_test = torchvision.datasets.MNIST(mnist_path, train=False, download=True, transform = mnist_test)\n",
        "\n",
        "  mnist_train_loader = DataLoader(mnist_train, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  mnist_test_loader = DataLoader(mnist_test, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "  trainloaders.append(mnist_train_loader)\n",
        "  testloaders.append(mnist_test_loader)\n",
        "\n",
        "  #Load CIFAR100\n",
        "  cifar_train = torchvision.datasets.CIFAR100(cifar_path, train=True, download=True, transform=train_transform)\n",
        "  cifar_test = torchvision.datasets.CIFAR100(cifar_path, train=False, download=True, transform=test_transform)\n",
        "\n",
        "  cifar_train_loader = DataLoader(cifar_train, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  cifar_test_loader = DataLoader(cifar_test, batch_size = batch_size, shuffle = False)\n",
        "\n",
        "  trainloaders.append(cifar_train_loader)\n",
        "  testloaders.append(cifar_test_loader)\n",
        "\n",
        "  # Load Flowers102\n",
        "  flowers_train = torchvision.datasets.Flowers102(flower_path, split=\"train\", download=True, transform=train_transform)\n",
        "  flowers_test = torchvision.datasets.Flowers102(flower_path, split=\"test\", download=True, transform=test_transform)\n",
        "\n",
        "  flowers_train_loader = DataLoader(flowers_train, batch_size = batch_size, shuffle = True, num_workers = 2)\n",
        "  flowers_test_loader = DataLoader(flowers_test, batch_size = batch_size, shuffle = False)\n",
        "  \n",
        "  trainloaders.append(flowers_train_loader)\n",
        "  testloaders.append(flowers_test_loader)\n",
        "\n",
        "  return trainloaders, testloaders"
      ],
      "metadata": {
        "id": "Kg_wsrepXh7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Baselines"
      ],
      "metadata": {
        "id": "0AYEovrNXl9R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune"
      ],
      "metadata": {
        "id": "0MgmBW6eXojX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Finetune:\n",
        "  def __init__(self, num_outputs, epochs, lr):\n",
        "\n",
        "    self.num_outputs = num_outputs\n",
        "\n",
        "    self.model = models.resnet34(weights = None)\n",
        "    self.model.fc = nn.Linear(self.model.fc.in_features, self.num_outputs)\n",
        "\n",
        "    self.epochs = epochs\n",
        "    self.lr = lr\n",
        "    self.lossFunc = nn.CrossEntropyLoss()\n",
        "    self.task_idx = 0\n",
        "\n",
        "  def train(self, trainloader, testloader):\n",
        "    self.task_idx += 1\n",
        "    self.model.train()\n",
        "    self.model.to(device)\n",
        "    optimizer = torch.optim.Adam(self.model.parameters(), lr = self.lr)\n",
        "\n",
        "    for epoch in range(self.epochs):      \n",
        "      for inputs, labels in tqdm(trainloader, desc = \"Training task {}, Epoch {}/{}\".format(self.task_idx, epoch + 1, self.epochs)):\n",
        "\n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels % self.num_outputs\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = self.model(inputs)\n",
        "        loss = self.lossFunc(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    self.eval(self.task_idx, testloader)\n",
        "  \n",
        "  def eval(self, task_idx, testloader):\n",
        "    eval_total = 0\n",
        "    eval_correct = 0\n",
        "    self.model.eval()\n",
        "    self.model.to(device)\n",
        "    for inputs, labels in tqdm(testloader, desc = \"Evaluating Task {}\".format(task_idx)):\n",
        "      \n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      labels = labels % self.num_outputs\n",
        "\n",
        "      outputs = self.model(inputs)\n",
        "\n",
        "      _, predictions = torch.max(outputs, dim = 1)\n",
        "      eval_total += labels.shape[0]\n",
        "      eval_correct += int((predictions == labels).sum()) \n",
        "    print(\"Evaluation Accucary on Task {} : {}\".format(task_idx, eval_correct/eval_total))"
      ],
      "metadata": {
        "id": "1WdYYXxHXoQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indivindual Networks"
      ],
      "metadata": {
        "id": "DRpbmtriX1n7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class IndivindualNets:\n",
        "  def __init__(self, num_models, num_outputs, epochs):\n",
        "    self.num_models = num_models\n",
        "\n",
        "    if isinstance(num_outputs, List):\n",
        "      self.num_outputs = num_outputs\n",
        "    else:\n",
        "      self.num_outputs = [num_outputs for _ in range(self.num_models)]\n",
        "\n",
        "    self.models = [models.resnet34(weights = None) for _ in range (self.num_models)]\n",
        "\n",
        "    #Replace the classfier for each model to match the number of classes of each task\n",
        "    for index, model in enumerate(self.models):\n",
        "      model.fc = nn.Linear(model.fc.in_features, self.num_outputs[index])\n",
        "    \n",
        "    self.epochs = epochs\n",
        "    self.lossFunc = nn.CrossEntropyLoss()\n",
        "    self.task_idx = 0\n",
        "  \n",
        "  def train(self, trainloader, testloader, lr):\n",
        "    self.task_idx += 1\n",
        "    model = self.models[self.task_idx - 1]\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr = lr)\n",
        "\n",
        "    for epoch in range(self.epochs):\n",
        "      for inputs, labels in tqdm(trainloader, desc = \"Training task {}, Epoch {}/{}\".format(self.task_idx, epoch + 1, self.epochs)):\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        labels = labels % self.num_outputs[self.task_idx - 1]\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "\n",
        "        loss = self.lossFunc(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    self.eval(self.task_idx, testloader)\n",
        "  \n",
        "  def eval(self, task_idx, testloader):\n",
        "    model = self.models[task_idx - 1]\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    eval_total = 0\n",
        "    eval_correct = 0\n",
        "    for inputs, labels in tqdm(testloader, desc = \"Evaluating Task {}\".format(task_idx)):\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "      labels = labels % self.num_outputs[task_idx - 1]\n",
        "\n",
        "      outputs = model(inputs)\n",
        "\n",
        "      _, predictions = torch.max(outputs, dim = 1)\n",
        "      eval_total += labels.shape[0]\n",
        "      eval_correct += int((predictions == labels).sum()) \n",
        "    print(\"Evaluation Accucary on Task {} : {}\".format(task_idx, eval_correct / eval_total))"
      ],
      "metadata": {
        "id": "yW-x5dvTX3w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CIFAR-100 Example"
      ],
      "metadata": {
        "id": "8yxXwFjkYA7G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_cifar100()\n",
        "num_outputs = 20\n",
        "lr = 1e-3\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "6cfINJBlYDLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune"
      ],
      "metadata": {
        "id": "7XE5xN5NYPUb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune = Finetune(num_outputs, epochs, lr)\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  finetune.train(trainloader, testloader)"
      ],
      "metadata": {
        "id": "dVxE_fAAYHSd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  finetune.eval(task_idx + 1, testloader)"
      ],
      "metadata": {
        "id": "qkvuZCq8YMrA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indivindual Networks"
      ],
      "metadata": {
        "id": "pGPlsMZ6YQzF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indNets = IndivindualNets(5, num_outputs, epochs)\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  indNets.train(trainloader, testloader, lr)"
      ],
      "metadata": {
        "id": "bG11dS1eYUQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  indNets.eval(task_idx + 1, testloader)"
      ],
      "metadata": {
        "id": "2FXggoSYYUwl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Permuted MNIST Example"
      ],
      "metadata": {
        "id": "fSGq60PrYOGI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_permuted_mnist()\n",
        "num_outputs = 10\n",
        "lr = 1e-3\n",
        "epochs = 5"
      ],
      "metadata": {
        "id": "wP8x7t70YgeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune"
      ],
      "metadata": {
        "id": "jOtKiun9ZE9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune = Finetune(num_outputs, epochs, lr)\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  finetune.train(trainloader, testloader)"
      ],
      "metadata": {
        "id": "lRdH7P7sZEfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  finetune.eval(task_idx + 1, testloader)"
      ],
      "metadata": {
        "id": "WHOhH3PmZMnV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indivindual Networks"
      ],
      "metadata": {
        "id": "GwsRNz--ZGvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indNets = IndivindualNets(5, num_outputs, epochs)\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  indNets.train(trainloader, testloader, lr)"
      ],
      "metadata": {
        "id": "FL3zcwEVZJRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  indNets.eval(task_idx + 1, testloader)"
      ],
      "metadata": {
        "id": "52cxDKK-ZUfS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Flowers-102 Example"
      ],
      "metadata": {
        "id": "bonRJdQjZYCK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_flowers102()\n",
        "num_outputs = 17\n",
        "lr = 1e-3\n",
        "epochs = 20"
      ],
      "metadata": {
        "id": "UHWUaPK8Zjuv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune"
      ],
      "metadata": {
        "id": "Mav-voQoZeyr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune = Finetune(num_outputs, epochs, lr)\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  finetune.train(trainloader, testloader)"
      ],
      "metadata": {
        "id": "ZcjGCphgZeU_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  finetune.eval(task_idx + 1, testloader)"
      ],
      "metadata": {
        "id": "H9eaybPOZr62",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db8bf87d-5172-4e6f-b834-8773ceca2863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 1: 100%|██████████| 40/40 [00:06<00:00,  5.89it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 1 : 0.06050955414012739\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 2: 100%|██████████| 39/39 [00:07<00:00,  5.19it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 2 : 0.052202283849918436\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 3: 100%|██████████| 77/77 [00:14<00:00,  5.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 3 : 0.021103896103896104\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 4: 100%|██████████| 57/57 [00:10<00:00,  5.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 4 : 0.05077262693156733\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 5: 100%|██████████| 103/103 [00:18<00:00,  5.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 5 : 0.09363525091799266\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 6: 100%|██████████| 71/71 [00:12<00:00,  5.46it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 6 : 0.29313380281690143\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indivindual Networks"
      ],
      "metadata": {
        "id": "flAyyT4CZhBJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "indNets = IndivindualNets(6, num_outputs, epochs)\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  indNets.train(trainloader, testloader, 1e-4)"
      ],
      "metadata": {
        "id": "Tl1r6eaIZjFx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e9108f3-d1dd-4aaf-f0e1-f753ec6ca8ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training task 1, Epoch 1/20: 100%|██████████| 11/11 [00:02<00:00,  5.27it/s]\n",
            "Training task 1, Epoch 2/20: 100%|██████████| 11/11 [00:01<00:00,  6.02it/s]\n",
            "Training task 1, Epoch 3/20: 100%|██████████| 11/11 [00:01<00:00,  6.32it/s]\n",
            "Training task 1, Epoch 4/20: 100%|██████████| 11/11 [00:02<00:00,  4.73it/s]\n",
            "Training task 1, Epoch 5/20: 100%|██████████| 11/11 [00:02<00:00,  4.70it/s]\n",
            "Training task 1, Epoch 6/20: 100%|██████████| 11/11 [00:01<00:00,  6.13it/s]\n",
            "Training task 1, Epoch 7/20: 100%|██████████| 11/11 [00:01<00:00,  6.19it/s]\n",
            "Training task 1, Epoch 8/20: 100%|██████████| 11/11 [00:01<00:00,  6.33it/s]\n",
            "Training task 1, Epoch 9/20: 100%|██████████| 11/11 [00:01<00:00,  6.27it/s]\n",
            "Training task 1, Epoch 10/20: 100%|██████████| 11/11 [00:01<00:00,  6.44it/s]\n",
            "Training task 1, Epoch 11/20: 100%|██████████| 11/11 [00:02<00:00,  3.91it/s]\n",
            "Training task 1, Epoch 12/20: 100%|██████████| 11/11 [00:01<00:00,  5.99it/s]\n",
            "Training task 1, Epoch 13/20: 100%|██████████| 11/11 [00:01<00:00,  6.39it/s]\n",
            "Training task 1, Epoch 14/20: 100%|██████████| 11/11 [00:01<00:00,  6.30it/s]\n",
            "Training task 1, Epoch 15/20: 100%|██████████| 11/11 [00:01<00:00,  6.28it/s]\n",
            "Training task 1, Epoch 16/20: 100%|██████████| 11/11 [00:01<00:00,  6.30it/s]\n",
            "Training task 1, Epoch 17/20: 100%|██████████| 11/11 [00:01<00:00,  6.03it/s]\n",
            "Training task 1, Epoch 18/20: 100%|██████████| 11/11 [00:02<00:00,  3.80it/s]\n",
            "Training task 1, Epoch 19/20: 100%|██████████| 11/11 [00:01<00:00,  6.42it/s]\n",
            "Training task 1, Epoch 20/20: 100%|██████████| 11/11 [00:01<00:00,  6.47it/s]\n",
            "Evaluating Task 1: 100%|██████████| 40/40 [00:06<00:00,  5.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 1 : 0.4856687898089172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training task 2, Epoch 1/20: 100%|██████████| 11/11 [00:02<00:00,  4.40it/s]\n",
            "Training task 2, Epoch 2/20: 100%|██████████| 11/11 [00:01<00:00,  6.47it/s]\n",
            "Training task 2, Epoch 3/20: 100%|██████████| 11/11 [00:01<00:00,  6.34it/s]\n",
            "Training task 2, Epoch 4/20: 100%|██████████| 11/11 [00:01<00:00,  6.48it/s]\n",
            "Training task 2, Epoch 5/20: 100%|██████████| 11/11 [00:01<00:00,  6.41it/s]\n",
            "Training task 2, Epoch 6/20: 100%|██████████| 11/11 [00:01<00:00,  6.31it/s]\n",
            "Training task 2, Epoch 7/20: 100%|██████████| 11/11 [00:02<00:00,  4.33it/s]\n",
            "Training task 2, Epoch 8/20: 100%|██████████| 11/11 [00:02<00:00,  5.37it/s]\n",
            "Training task 2, Epoch 9/20: 100%|██████████| 11/11 [00:01<00:00,  6.04it/s]\n",
            "Training task 2, Epoch 10/20: 100%|██████████| 11/11 [00:01<00:00,  6.57it/s]\n",
            "Training task 2, Epoch 11/20: 100%|██████████| 11/11 [00:01<00:00,  6.31it/s]\n",
            "Training task 2, Epoch 12/20: 100%|██████████| 11/11 [00:01<00:00,  6.35it/s]\n",
            "Training task 2, Epoch 13/20: 100%|██████████| 11/11 [00:01<00:00,  6.23it/s]\n",
            "Training task 2, Epoch 14/20: 100%|██████████| 11/11 [00:02<00:00,  3.74it/s]\n",
            "Training task 2, Epoch 15/20: 100%|██████████| 11/11 [00:01<00:00,  6.60it/s]\n",
            "Training task 2, Epoch 16/20: 100%|██████████| 11/11 [00:01<00:00,  6.45it/s]\n",
            "Training task 2, Epoch 17/20: 100%|██████████| 11/11 [00:01<00:00,  6.58it/s]\n",
            "Training task 2, Epoch 18/20: 100%|██████████| 11/11 [00:01<00:00,  6.49it/s]\n",
            "Training task 2, Epoch 19/20: 100%|██████████| 11/11 [00:01<00:00,  6.51it/s]\n",
            "Training task 2, Epoch 20/20: 100%|██████████| 11/11 [00:01<00:00,  6.38it/s]\n",
            "Evaluating Task 2: 100%|██████████| 39/39 [00:07<00:00,  5.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 2 : 0.42251223491027734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training task 3, Epoch 1/20: 100%|██████████| 11/11 [00:01<00:00,  6.27it/s]\n",
            "Training task 3, Epoch 2/20: 100%|██████████| 11/11 [00:01<00:00,  6.37it/s]\n",
            "Training task 3, Epoch 3/20: 100%|██████████| 11/11 [00:01<00:00,  5.66it/s]\n",
            "Training task 3, Epoch 4/20: 100%|██████████| 11/11 [00:02<00:00,  4.16it/s]\n",
            "Training task 3, Epoch 5/20: 100%|██████████| 11/11 [00:01<00:00,  6.39it/s]\n",
            "Training task 3, Epoch 6/20: 100%|██████████| 11/11 [00:01<00:00,  6.38it/s]\n",
            "Training task 3, Epoch 7/20: 100%|██████████| 11/11 [00:01<00:00,  6.33it/s]\n",
            "Training task 3, Epoch 8/20: 100%|██████████| 11/11 [00:01<00:00,  6.18it/s]\n",
            "Training task 3, Epoch 9/20: 100%|██████████| 11/11 [00:01<00:00,  6.28it/s]\n",
            "Training task 3, Epoch 10/20: 100%|██████████| 11/11 [00:02<00:00,  4.41it/s]\n",
            "Training task 3, Epoch 11/20: 100%|██████████| 11/11 [00:02<00:00,  5.09it/s]\n",
            "Training task 3, Epoch 12/20: 100%|██████████| 11/11 [00:01<00:00,  6.38it/s]\n",
            "Training task 3, Epoch 13/20: 100%|██████████| 11/11 [00:01<00:00,  6.28it/s]\n",
            "Training task 3, Epoch 14/20: 100%|██████████| 11/11 [00:01<00:00,  6.34it/s]\n",
            "Training task 3, Epoch 15/20: 100%|██████████| 11/11 [00:01<00:00,  6.31it/s]\n",
            "Training task 3, Epoch 16/20: 100%|██████████| 11/11 [00:01<00:00,  6.36it/s]\n",
            "Training task 3, Epoch 17/20: 100%|██████████| 11/11 [00:02<00:00,  3.75it/s]\n",
            "Training task 3, Epoch 18/20: 100%|██████████| 11/11 [00:01<00:00,  5.98it/s]\n",
            "Training task 3, Epoch 19/20: 100%|██████████| 11/11 [00:01<00:00,  6.36it/s]\n",
            "Training task 3, Epoch 20/20: 100%|██████████| 11/11 [00:01<00:00,  6.42it/s]\n",
            "Evaluating Task 3: 100%|██████████| 77/77 [00:14<00:00,  5.43it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 3 : 0.4180194805194805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training task 4, Epoch 1/20: 100%|██████████| 11/11 [00:01<00:00,  6.19it/s]\n",
            "Training task 4, Epoch 2/20: 100%|██████████| 11/11 [00:01<00:00,  5.87it/s]\n",
            "Training task 4, Epoch 3/20: 100%|██████████| 11/11 [00:02<00:00,  4.04it/s]\n",
            "Training task 4, Epoch 4/20: 100%|██████████| 11/11 [00:01<00:00,  6.27it/s]\n",
            "Training task 4, Epoch 5/20: 100%|██████████| 11/11 [00:01<00:00,  6.28it/s]\n",
            "Training task 4, Epoch 6/20: 100%|██████████| 11/11 [00:01<00:00,  6.11it/s]\n",
            "Training task 4, Epoch 7/20: 100%|██████████| 11/11 [00:01<00:00,  6.31it/s]\n",
            "Training task 4, Epoch 8/20: 100%|██████████| 11/11 [00:01<00:00,  6.24it/s]\n",
            "Training task 4, Epoch 9/20: 100%|██████████| 11/11 [00:02<00:00,  4.60it/s]\n",
            "Training task 4, Epoch 10/20: 100%|██████████| 11/11 [00:02<00:00,  4.81it/s]\n",
            "Training task 4, Epoch 11/20: 100%|██████████| 11/11 [00:01<00:00,  6.16it/s]\n",
            "Training task 4, Epoch 12/20: 100%|██████████| 11/11 [00:01<00:00,  6.34it/s]\n",
            "Training task 4, Epoch 13/20: 100%|██████████| 11/11 [00:01<00:00,  6.47it/s]\n",
            "Training task 4, Epoch 14/20: 100%|██████████| 11/11 [00:01<00:00,  6.29it/s]\n",
            "Training task 4, Epoch 15/20: 100%|██████████| 11/11 [00:01<00:00,  6.30it/s]\n",
            "Training task 4, Epoch 16/20: 100%|██████████| 11/11 [00:02<00:00,  3.80it/s]\n",
            "Training task 4, Epoch 17/20: 100%|██████████| 11/11 [00:01<00:00,  6.20it/s]\n",
            "Training task 4, Epoch 18/20: 100%|██████████| 11/11 [00:01<00:00,  6.45it/s]\n",
            "Training task 4, Epoch 19/20: 100%|██████████| 11/11 [00:01<00:00,  6.43it/s]\n",
            "Training task 4, Epoch 20/20: 100%|██████████| 11/11 [00:01<00:00,  6.36it/s]\n",
            "Evaluating Task 4: 100%|██████████| 57/57 [00:10<00:00,  5.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 4 : 0.7240618101545254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training task 5, Epoch 1/20: 100%|██████████| 11/11 [00:01<00:00,  6.37it/s]\n",
            "Training task 5, Epoch 2/20: 100%|██████████| 11/11 [00:01<00:00,  6.29it/s]\n",
            "Training task 5, Epoch 3/20: 100%|██████████| 11/11 [00:01<00:00,  6.17it/s]\n",
            "Training task 5, Epoch 4/20: 100%|██████████| 11/11 [00:02<00:00,  3.93it/s]\n",
            "Training task 5, Epoch 5/20: 100%|██████████| 11/11 [00:01<00:00,  6.48it/s]\n",
            "Training task 5, Epoch 6/20: 100%|██████████| 11/11 [00:01<00:00,  6.34it/s]\n",
            "Training task 5, Epoch 7/20: 100%|██████████| 11/11 [00:01<00:00,  6.24it/s]\n",
            "Training task 5, Epoch 8/20: 100%|██████████| 11/11 [00:01<00:00,  6.33it/s]\n",
            "Training task 5, Epoch 9/20: 100%|██████████| 11/11 [00:01<00:00,  6.31it/s]\n",
            "Training task 5, Epoch 10/20: 100%|██████████| 11/11 [00:02<00:00,  5.18it/s]\n",
            "Training task 5, Epoch 11/20: 100%|██████████| 11/11 [00:02<00:00,  4.71it/s]\n",
            "Training task 5, Epoch 12/20: 100%|██████████| 11/11 [00:01<00:00,  6.33it/s]\n",
            "Training task 5, Epoch 13/20: 100%|██████████| 11/11 [00:01<00:00,  6.59it/s]\n",
            "Training task 5, Epoch 14/20: 100%|██████████| 11/11 [00:01<00:00,  6.32it/s]\n",
            "Training task 5, Epoch 15/20: 100%|██████████| 11/11 [00:01<00:00,  6.47it/s]\n",
            "Training task 5, Epoch 16/20: 100%|██████████| 11/11 [00:01<00:00,  6.29it/s]\n",
            "Training task 5, Epoch 17/20: 100%|██████████| 11/11 [00:02<00:00,  4.41it/s]\n",
            "Training task 5, Epoch 18/20: 100%|██████████| 11/11 [00:02<00:00,  5.20it/s]\n",
            "Training task 5, Epoch 19/20: 100%|██████████| 11/11 [00:01<00:00,  6.51it/s]\n",
            "Training task 5, Epoch 20/20: 100%|██████████| 11/11 [00:01<00:00,  6.33it/s]\n",
            "Evaluating Task 5: 100%|██████████| 103/103 [00:18<00:00,  5.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 5 : 0.3812729498164015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training task 6, Epoch 1/20: 100%|██████████| 11/11 [00:02<00:00,  4.17it/s]\n",
            "Training task 6, Epoch 2/20: 100%|██████████| 11/11 [00:01<00:00,  6.31it/s]\n",
            "Training task 6, Epoch 3/20: 100%|██████████| 11/11 [00:01<00:00,  6.42it/s]\n",
            "Training task 6, Epoch 4/20: 100%|██████████| 11/11 [00:01<00:00,  6.50it/s]\n",
            "Training task 6, Epoch 5/20: 100%|██████████| 11/11 [00:01<00:00,  6.47it/s]\n",
            "Training task 6, Epoch 6/20: 100%|██████████| 11/11 [00:01<00:00,  6.60it/s]\n",
            "Training task 6, Epoch 7/20: 100%|██████████| 11/11 [00:02<00:00,  5.29it/s]\n",
            "Training task 6, Epoch 8/20: 100%|██████████| 11/11 [00:02<00:00,  4.44it/s]\n",
            "Training task 6, Epoch 9/20: 100%|██████████| 11/11 [00:01<00:00,  6.32it/s]\n",
            "Training task 6, Epoch 10/20: 100%|██████████| 11/11 [00:01<00:00,  6.43it/s]\n",
            "Training task 6, Epoch 11/20: 100%|██████████| 11/11 [00:01<00:00,  6.39it/s]\n",
            "Training task 6, Epoch 12/20: 100%|██████████| 11/11 [00:01<00:00,  6.42it/s]\n",
            "Training task 6, Epoch 13/20: 100%|██████████| 11/11 [00:01<00:00,  6.55it/s]\n",
            "Training task 6, Epoch 14/20: 100%|██████████| 11/11 [00:02<00:00,  4.93it/s]\n",
            "Training task 6, Epoch 15/20: 100%|██████████| 11/11 [00:02<00:00,  4.67it/s]\n",
            "Training task 6, Epoch 16/20: 100%|██████████| 11/11 [00:01<00:00,  6.24it/s]\n",
            "Training task 6, Epoch 17/20: 100%|██████████| 11/11 [00:01<00:00,  6.38it/s]\n",
            "Training task 6, Epoch 18/20: 100%|██████████| 11/11 [00:01<00:00,  6.46it/s]\n",
            "Training task 6, Epoch 19/20: 100%|██████████| 11/11 [00:01<00:00,  6.42it/s]\n",
            "Training task 6, Epoch 20/20: 100%|██████████| 11/11 [00:01<00:00,  6.63it/s]\n",
            "Evaluating Task 6: 100%|██████████| 71/71 [00:12<00:00,  5.54it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 6 : 0.3829225352112676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  indNets.eval(task_idx + 1, testloader)"
      ],
      "metadata": {
        "id": "9tQGnD8AZwKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e72788b0-b90e-40be-9959-b8743964ccd3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 1: 100%|██████████| 40/40 [00:07<00:00,  5.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 1 : 0.4856687898089172\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 2: 100%|██████████| 39/39 [00:07<00:00,  5.29it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 2 : 0.42251223491027734\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 3: 100%|██████████| 77/77 [00:14<00:00,  5.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 3 : 0.4180194805194805\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 4: 100%|██████████| 57/57 [00:10<00:00,  5.33it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 4 : 0.7240618101545254\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 5: 100%|██████████| 103/103 [00:18<00:00,  5.60it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 5 : 0.3812729498164015\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating Task 6: 100%|██████████| 71/71 [00:12<00:00,  5.48it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Accucary on Task 6 : 0.3829225352112676\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Diff Datasets Example"
      ],
      "metadata": {
        "id": "XG3CYTN9ZzOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainloaders, testloaders = get_diff_datasets()\n",
        "num_outputs = [10, 100, 102]\n",
        "lr = 1e-3\n",
        "epochs = 10"
      ],
      "metadata": {
        "id": "XL1EizsGZ2WJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Finetune"
      ],
      "metadata": {
        "id": "fA0zv6WnZ6Sn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "finetune = Finetune(num_outputs[-1], epochs, lr)\n",
        "for trainloader, testloader in zip(trainloaders, testloaders):\n",
        "  finetune.train(trainloader, testloader)"
      ],
      "metadata": {
        "id": "y1HrzNsEZ5gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  finetune.eval(task_idx + 1, testloader)"
      ],
      "metadata": {
        "id": "TgnlZvkcaG6b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Indivindual Networks"
      ],
      "metadata": {
        "id": "vZFO3osKZ70b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lr = [1e-3, 1e-3, 1e-4]\n",
        "indNets = IndivindualNets(3, num_outputs, epochs)\n",
        "for trainloader, testloader, lr in zip(trainloaders, testloaders, lr):\n",
        "  indNets.train(trainloader, testloader, lr)"
      ],
      "metadata": {
        "id": "gUvwnDbbaJhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for task_idx, testloader in enumerate(testloaders):\n",
        "  indNets.eval(task_idx + 1, testloader)"
      ],
      "metadata": {
        "id": "A0-qYlM6aMOg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}